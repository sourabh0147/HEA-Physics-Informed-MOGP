{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gpytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tdP0nHUtvvv",
        "outputId": "c74d002b-9176-410b-e03b-a8dacabdc4e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.14.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jaxtyping (from gpytorch)\n",
            "  Downloading jaxtyping-0.3.4-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.12/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from gpytorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from gpytorch) (1.16.3)\n",
            "Collecting linear_operator>=0.6 (from gpytorch)\n",
            "  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from linear_operator>=0.6->gpytorch) (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy>=1.6.0->gpytorch) (2.0.2)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->gpytorch) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->gpytorch) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear_operator>=0.6->gpytorch) (2025.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->linear_operator>=0.6->gpytorch) (3.0.3)\n",
            "Downloading gpytorch-1.14.3-py3-none-any.whl (280 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.6/280.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.4-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, jaxtyping, linear_operator, gpytorch\n",
            "Successfully installed gpytorch-1.14.3 jaxtyping-0.3.4 linear_operator-0.6 wadler-lindig-0.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3mIPS0ytRYW",
        "outputId": "d5b8b751-17c8-4a23-f5b1-0e9b4391e9f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MOGP Model (500 Epochs)...\n",
            "Epoch 100: Loss = -0.697\n",
            "Epoch 200: Loss = -0.751\n",
            "Epoch 300: Loss = -0.759\n",
            "Epoch 400: Loss = -0.766\n",
            "Epoch 500: Loss = -0.771\n",
            "\n",
            "SUCCESS: 'mogp_assets.pkl' created.\n",
            "You can now run MOGP-based inference scripts.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gpytorch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import joblib\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# ==========================================\n",
        "# 1. DATA INGESTION & PHYSICS EMBEDDING\n",
        "# ==========================================\n",
        "# (Copied strictly from digital_twin_files.ipynb)\n",
        "csv_content = \"\"\"Run,Material,Power_W/mm^2,Speed_mm/s,Radius_mm,Y1_Max_Prin_Stress (MPa),Y2_Max_Von_Mises,Y3_Deformation mm,Max Temp,Heat flux\n",
        "1,CoCrFeNiMn ,137.27,5.31,1.44,355.95,1360.2,0.0172,255.24,0.4193\n",
        "2,CoCrFeNiMn ,260.59,39.51,1.11,43.22,215.36,0.0027,58.48,0.0933\n",
        "3,CoCrFeNiMn ,183.35,21.1,1.22,70.385,339.24,0.0042,79.584,0.1412\n",
        "4,CoCrFeNiMn ,56.31,32.79,0.89,7.3,36.37,0.0005,28.17,0.0158\n",
        "5,CoCrFeNiMn ,86.3,22.69,1.06,22.986,112.5,0.0014,41.09,0.0474\n",
        "6,CoCrFeNiMn ,217.8,33.32,1.4,67.52,336.46,0.0042,78.97,0.1449\n",
        "7,CoCrFeNiMn ,289.23,16.03,0.92,86.55,403.25,0.005,90.65,0.1651\n",
        "8,CoCrFeNiMn ,169.14,49.17,1.27,29.557,147.42,0.0018,46.94,0.0649\n",
        "9,CoCrFeNiMn ,147.07,16.71,0.94,43.89,204.7,0.0025,56.82,0.0838\n",
        "10,CoCrFeNiMn ,282.05,28.76,1.29,0,0,0,22,0\n",
        "11,AlCoCrFeNi,137.27,5.31,1.44,405.6,1982.7,0.031,254.83,0.4175\n",
        "12,AlCoCrFeNi,260.59,39.51,1.11,57.721,310.92,0.005,58.41,0.093\n",
        "13,AlCoCrFeNi,183.35,21.1,1.22,90.92,490.46,0.0078,79.47,0.1407\n",
        "14,AlCoCrFeNi,56.31,32.79,0.89,9.65,51.98,0.0008,28.09,0.0156\n",
        "15,AlCoCrFeNi,86.3,22.69,1.06,30.148,162.61,0.0026,41.06,0.0472\n",
        "16,AlCoCrFeNi,217.8,33.32,1.4,90.2,485.68,0.0078,78.86,0.1444\n",
        "17,AlCoCrFeNi,289.23,16.03,0.92,108.03,583.96,0.0092,90.52,0.1645\n",
        "18,AlCoCrFeNi,169.14,49.17,1.27,39.49,212.61,0.0034,46.9,0.0646\n",
        "19,AlCoCrFeNi,147.07,16.71,0.94,58.84,296.28,0.0047,56.75,0.0835\n",
        "20,AlCoCrFeNi,282.05,28.76,1.29,0,0,0,22,0\n",
        "21,FeCrNiTiAl,137.27,5.31,1.44,404.89,1980.9,0.0311,254.88,0.4171\n",
        "22,FeCrNiTiAl,260.59,39.51,1.11,54.462,310.64,0.005,58.42,0.093\n",
        "23,FeCrNiTiAl,183.35,21.1,1.22,58.788,490.1,0.0078,79.48,0.1406\n",
        "24,FeCrNiTiAl,56.31,32.79,0.89,9.1938,52.51,0.0008,28.16,0.0158\n",
        "25,FeCrNiTiAl,86.3,22.69,1.06,28.446,162.46,0.0026,41.06,0.0472\n",
        "26,FeCrNiTiAl,217.8,33.32,1.4,85.11,485.24,0.0078,78.87,0.1444\n",
        "27,FeCrNiTiAl,289.23,16.03,0.92,101.94,583.43,0.0093,90.532,0.1644\n",
        "28,FeCrNiTiAl,169.14,49.17,1.27,37.258,212.41,0.0034,46.9,0.0646\n",
        "29,FeCrNiTiAl,147.07,16.71,0.94,51.748,296.01,0.0047,56.76,0.0834\n",
        "30,FeCrNiTiAl,282.05,28.76,1.29,0,0,0,22,0\n",
        "31,NbSiTaTiZr,137.27,5.31,1.44,402.62,1985.1,0.0309,255.76,0.4206\n",
        "32,NbSiTaTiZr,260.59,39.51,1.11,53.92,311.33,0.0049,58.57,0.0936\n",
        "33,NbSiTaTiZr,183.35,21.1,1.22,70.41,407.24,0.0065,69.87,0.1175\n",
        "34,NbSiTaTiZr,56.31,32.79,0.89,9.01,52.05,0.0008,28.12,0.0157\n",
        "35,NbSiTaTiZr,86.3,22.69,1.06,24.83,134.91,0.0022,37.77,0.0326\n",
        "36,NbSiTaTiZr,217.8,33.32,1.4,84.27,486.33,0.0077,79.1,0.1453\n",
        "37,NbSiTaTiZr,289.23,16.03,0.92,100.84,584.51,0.0092,90.78,0.1654\n",
        "38,NbSiTaTiZr,169.14,49.17,1.27,36.89,212.89,0.0034,47,0.065\n",
        "39,NbSiTaTiZr,147.07,16.71,0.94,51.19,296.55,0.0047,56.88,0.0839\n",
        "40,NbSiTaTiZr,282.05,28.76,1.29,0,0,0,22,0\n",
        "41,HfNbTiZr,137.27,5.31,1.44,404.99,1990.7,0.0311,256.07,0.4218\n",
        "42,HfNbTiZr,260.59,39.51,1.11,55.88,312.25,0.005,58.62,0.0938\n",
        "43,HfNbTiZr,183.35,21.1,1.22,87.98,492.33,0.0078,79.77,0.1418\n",
        "44,HfNbTiZr,56.31,32.79,0.89,9.34,52.2,0.0008,28.12,0.0157\n",
        "45,HfNbTiZr,86.3,22.69,1.06,29.182,163.3,0.0026,41.165,0.0476\n",
        "46,HfNbTiZr,217.8,33.32,1.4,87.193,487.08,0.0078,79.1,0.1455\n",
        "47,HfNbTiZr,289.23,16.03,0.92,104.53,586.2,0.0093,90.88,0.1658\n",
        "48,HfNbTiZr,169.14,49.17,1.27,38.227,213.52,0.0034,47.04,0.0652\n",
        "49,HfNbTiZr,147.07,16.71,0.94,53.06,297.41,0.0047,56.93,0.0841\n",
        "50,HfNbTiZr,282.05,28.76,1.29,0,0,0,22,0\n",
        "\"\"\"\n",
        "df = pd.read_csv(io.StringIO(csv_content))\n",
        "df['Material'] = df['Material'].str.strip()\n",
        "\n",
        "# Clean Data\n",
        "df_clean = df[(df['Y2_Max_Von_Mises'] > 1e-6) & (df['Heat flux'] > 1e-6)].copy()\n",
        "\n",
        "# Feature Engineering (Physics Embedding)\n",
        "process_params = ['Power_W/mm^2', 'Speed_mm/s', 'Radius_mm']\n",
        "output_cols = ['Y1_Max_Prin_Stress (MPa)', 'Y2_Max_Von_Mises', 'Y3_Deformation mm', 'Max Temp', 'Heat flux']\n",
        "\n",
        "material_props = {\n",
        "    'CoCrFeNiMn': [8250, 12.5, 425],\n",
        "    'AlCoCrFeNi': [8000, 15.0, 445],\n",
        "    'FeCrNiTiAl': [8100, 15.0, 435],\n",
        "    'NbSiTaTiZr': [8350, 10.0, 405],\n",
        "    'HfNbTiZr':   [8450, 8.5,  395]\n",
        "}\n",
        "\n",
        "props_df = pd.DataFrame(\n",
        "    df_clean['Material'].map(material_props).tolist(),\n",
        "    index=df_clean.index, columns=['Density', 'Conductivity', 'Cp']\n",
        ")\n",
        "\n",
        "X_data = pd.concat([df_clean[process_params], props_df], axis=1)\n",
        "y_data = df_clean[output_cols]\n",
        "\n",
        "# Normalization\n",
        "X_scaler = MinMaxScaler()\n",
        "y_scaler = StandardScaler()\n",
        "X_scaled = X_scaler.fit_transform(X_data)\n",
        "y_scaled = y_scaler.fit_transform(y_data)\n",
        "\n",
        "train_x = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "train_y = torch.tensor(y_scaled, dtype=torch.float32)\n",
        "\n",
        "# ==========================================\n",
        "# 2. MODEL DEFINITION\n",
        "# ==========================================\n",
        "class MOGP_Surrogate(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(MOGP_Surrogate, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.MultitaskMean(\n",
        "            gpytorch.means.ConstantMean(), num_tasks=5\n",
        "        )\n",
        "        data_covar_module = gpytorch.kernels.ScaleKernel(\n",
        "            gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=train_x.size(-1))\n",
        "        )\n",
        "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
        "            data_covar_module, num_tasks=5, rank=1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "# ==========================================\n",
        "# 3. TRAINING\n",
        "# ==========================================\n",
        "print(\"Training MOGP Model (500 Epochs)...\")\n",
        "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=5)\n",
        "model = MOGP_Surrogate(train_x, train_y, likelihood)\n",
        "\n",
        "model.train()\n",
        "likelihood.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
        "\n",
        "for i in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(train_x)\n",
        "    loss = -mll(output, train_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (i+1) % 100 == 0:\n",
        "        print(f\"Epoch {i+1}: Loss = {loss.item():.3f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. EXPORT ASSETS\n",
        "# ==========================================\n",
        "# We save the 'state_dict' (best practice for PyTorch) and everything needed to rebuild the model\n",
        "export_package = {\n",
        "    'model_state': model.state_dict(),\n",
        "    'likelihood_state': likelihood.state_dict(),\n",
        "    'materials': material_props,\n",
        "    'train_x': train_x,  # Required to init ExactGP\n",
        "    'train_y': train_y,  # Required to init ExactGP\n",
        "    'X_scaler': X_scaler,\n",
        "    'y_scaler': y_scaler\n",
        "}\n",
        "\n",
        "joblib.dump(export_package, 'mogp_assets.pkl')\n",
        "print(\"\\nSUCCESS: 'mogp_assets.pkl' created.\")\n",
        "print(\"You can now run MOGP-based inference scripts.\")"
      ]
    }
  ]
}